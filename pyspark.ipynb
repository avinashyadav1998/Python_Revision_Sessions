{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install pyspark","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:13:00.780579Z","iopub.execute_input":"2022-04-18T10:13:00.781155Z","iopub.status.idle":"2022-04-18T10:13:48.844915Z","shell.execute_reply.started":"2022-04-18T10:13:00.781101Z","shell.execute_reply":"2022-04-18T10:13:48.843947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install findspark","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:39:15.888489Z","iopub.execute_input":"2022-04-18T12:39:15.888815Z","iopub.status.idle":"2022-04-18T12:39:15.893338Z","shell.execute_reply.started":"2022-04-18T12:39:15.888781Z","shell.execute_reply":"2022-04-18T12:39:15.892546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import findspark\nfindspark.init","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:18:04.493843Z","iopub.execute_input":"2022-04-18T10:18:04.494155Z","iopub.status.idle":"2022-04-18T10:18:04.505093Z","shell.execute_reply.started":"2022-04-18T10:18:04.494114Z","shell.execute_reply":"2022-04-18T10:18:04.504381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:13:53.002432Z","iopub.execute_input":"2022-04-18T10:13:53.00312Z","iopub.status.idle":"2022-04-18T10:13:53.007629Z","shell.execute_reply.started":"2022-04-18T10:13:53.003082Z","shell.execute_reply":"2022-04-18T10:13:53.006546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-04-19T09:01:52.17015Z","iopub.execute_input":"2022-04-19T09:01:52.170522Z","iopub.status.idle":"2022-04-19T09:01:52.198922Z","shell.execute_reply.started":"2022-04-19T09:01:52.170427Z","shell.execute_reply":"2022-04-19T09:01:52.197804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_p = pd.read_csv('../input/iris-flower-dataset/IRIS.csv')\ndf_p.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:28:47.078217Z","iopub.execute_input":"2022-04-18T10:28:47.078585Z","iopub.status.idle":"2022-04-18T10:28:47.088773Z","shell.execute_reply.started":"2022-04-18T10:28:47.078525Z","shell.execute_reply":"2022-04-18T10:28:47.087668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pyspark","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:18:37.281494Z","iopub.execute_input":"2022-04-18T10:18:37.28211Z","iopub.status.idle":"2022-04-18T10:18:37.286726Z","shell.execute_reply.started":"2022-04-18T10:18:37.282067Z","shell.execute_reply":"2022-04-18T10:18:37.28584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pyspark.sql import SparkSession # required to created a dataframe\nspark=SparkSession.builder.appName(\"Basics\").getOrCreate() \n\nimport pyspark.sql.functions","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:18:40.190363Z","iopub.execute_input":"2022-04-18T10:18:40.190664Z","iopub.status.idle":"2022-04-18T10:18:40.196027Z","shell.execute_reply.started":"2022-04-18T10:18:40.190632Z","shell.execute_reply":"2022-04-18T10:18:40.195487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = spark.read.csv('../input/iris-flower-dataset/IRIS.csv')\ndf.show(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:20:02.923423Z","iopub.execute_input":"2022-04-18T10:20:02.924273Z","iopub.status.idle":"2022-04-18T10:20:03.593084Z","shell.execute_reply.started":"2022-04-18T10:20:02.924234Z","shell.execute_reply":"2022-04-18T10:20:03.592143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark = spark.read.option('header','true').csv('../input/iris-flower-dataset/IRIS.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:21:36.503131Z","iopub.execute_input":"2022-04-18T10:21:36.503408Z","iopub.status.idle":"2022-04-18T10:21:36.794194Z","shell.execute_reply.started":"2022-04-18T10:21:36.503379Z","shell.execute_reply":"2022-04-18T10:21:36.79325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:21:48.876043Z","iopub.execute_input":"2022-04-18T10:21:48.876499Z","iopub.status.idle":"2022-04-18T10:21:49.01726Z","shell.execute_reply.started":"2022-04-18T10:21:48.876448Z","shell.execute_reply":"2022-04-18T10:21:49.016315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(df_pyspark))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:22:05.809768Z","iopub.execute_input":"2022-04-18T10:22:05.810432Z","iopub.status.idle":"2022-04-18T10:22:05.816226Z","shell.execute_reply.started":"2022-04-18T10:22:05.81039Z","shell.execute_reply":"2022-04-18T10:22:05.815225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:22:21.423401Z","iopub.execute_input":"2022-04-18T10:22:21.42372Z","iopub.status.idle":"2022-04-18T10:22:21.534519Z","shell.execute_reply.started":"2022-04-18T10:22:21.423685Z","shell.execute_reply":"2022-04-18T10:22:21.533677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basics Part1","metadata":{}},{"cell_type":"raw","source":"print(spark)","metadata":{}},{"cell_type":"code","source":"## read dataset\n\n# 1. \n\ndf_pyspark  = spark.read.option('header', 'true').csv('../input/iris-flower-dataset/IRIS.csv', inferSchema=True)\n\n# if we will not add inferSchema as true , then it will show all datatypes as string","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:27:56.654234Z","iopub.execute_input":"2022-04-18T10:27:56.654545Z","iopub.status.idle":"2022-04-18T10:27:57.047741Z","shell.execute_reply.started":"2022-04-18T10:27:56.654501Z","shell.execute_reply":"2022-04-18T10:27:57.046652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:27:59.064395Z","iopub.execute_input":"2022-04-18T10:27:59.065392Z","iopub.status.idle":"2022-04-18T10:27:59.071249Z","shell.execute_reply.started":"2022-04-18T10:27:59.065353Z","shell.execute_reply":"2022-04-18T10:27:59.070242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_p.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:29:03.369437Z","iopub.execute_input":"2022-04-18T10:29:03.369787Z","iopub.status.idle":"2022-04-18T10:29:03.404147Z","shell.execute_reply.started":"2022-04-18T10:29:03.369749Z","shell.execute_reply":"2022-04-18T10:29:03.403526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2.\ndf_pyspark1 = spark.read.csv('../input/iris-flower-dataset/IRIS.csv', header=True, inferSchema=True)\ndf_pyspark1.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:30:35.077367Z","iopub.execute_input":"2022-04-18T10:30:35.077651Z","iopub.status.idle":"2022-04-18T10:30:35.38227Z","shell.execute_reply.started":"2022-04-18T10:30:35.077621Z","shell.execute_reply":"2022-04-18T10:30:35.3814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark1.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:30:59.309807Z","iopub.execute_input":"2022-04-18T10:30:59.310343Z","iopub.status.idle":"2022-04-18T10:30:59.507881Z","shell.execute_reply.started":"2022-04-18T10:30:59.310311Z","shell.execute_reply":"2022-04-18T10:30:59.506928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(df_pyspark)\n# Dataframe is a data structure because we can perform various operations on a dataframe","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:31:57.955186Z","iopub.execute_input":"2022-04-18T10:31:57.955589Z","iopub.status.idle":"2022-04-18T10:31:57.962055Z","shell.execute_reply.started":"2022-04-18T10:31:57.955553Z","shell.execute_reply":"2022-04-18T10:31:57.961257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.columns","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:43:45.564654Z","iopub.execute_input":"2022-04-18T10:43:45.564933Z","iopub.status.idle":"2022-04-18T10:43:45.572181Z","shell.execute_reply.started":"2022-04-18T10:43:45.564903Z","shell.execute_reply":"2022-04-18T10:43:45.571524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select single column\ndf_pyspark.select('species').show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:55:00.960939Z","iopub.execute_input":"2022-04-18T10:55:00.96182Z","iopub.status.idle":"2022-04-18T10:55:01.07803Z","shell.execute_reply.started":"2022-04-18T10:55:00.961772Z","shell.execute_reply":"2022-04-18T10:55:01.077386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(df_pyspark.select('species'))","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:55:37.974549Z","iopub.execute_input":"2022-04-18T10:55:37.974863Z","iopub.status.idle":"2022-04-18T10:55:37.988904Z","shell.execute_reply.started":"2022-04-18T10:55:37.974829Z","shell.execute_reply":"2022-04-18T10:55:37.988335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try another way\n\ndf_pyspark['species']","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:56:38.221762Z","iopub.execute_input":"2022-04-18T10:56:38.222669Z","iopub.status.idle":"2022-04-18T10:56:38.255388Z","shell.execute_reply.started":"2022-04-18T10:56:38.222621Z","shell.execute_reply":"2022-04-18T10:56:38.254775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check datatypes\nprint(df_pyspark.dtypes)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:57:17.323309Z","iopub.execute_input":"2022-04-18T10:57:17.323655Z","iopub.status.idle":"2022-04-18T10:57:17.328878Z","shell.execute_reply.started":"2022-04-18T10:57:17.323619Z","shell.execute_reply":"2022-04-18T10:57:17.327959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.describe().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T10:57:49.402029Z","iopub.execute_input":"2022-04-18T10:57:49.402343Z","iopub.status.idle":"2022-04-18T10:57:49.887662Z","shell.execute_reply.started":"2022-04-18T10:57:49.40231Z","shell.execute_reply":"2022-04-18T10:57:49.886796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding new column in data frame\ndf_pyspark = df_pyspark.withColumn('Flower_sepal',df_pyspark['sepal_length']+df_pyspark['sepal_width'])\n# this is not a inplace activity","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:08:36.4398Z","iopub.execute_input":"2022-04-18T11:08:36.440084Z","iopub.status.idle":"2022-04-18T11:08:36.452184Z","shell.execute_reply.started":"2022-04-18T11:08:36.440053Z","shell.execute_reply":"2022-04-18T11:08:36.451469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:07:59.741573Z","iopub.execute_input":"2022-04-18T11:07:59.741831Z","iopub.status.idle":"2022-04-18T11:07:59.879011Z","shell.execute_reply.started":"2022-04-18T11:07:59.741803Z","shell.execute_reply":"2022-04-18T11:07:59.878219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:09:12.774995Z","iopub.execute_input":"2022-04-18T11:09:12.775302Z","iopub.status.idle":"2022-04-18T11:09:12.894326Z","shell.execute_reply.started":"2022-04-18T11:09:12.775271Z","shell.execute_reply":"2022-04-18T11:09:12.893483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  drop columns\ndf_pyspark = df_pyspark.drop('Flower_sepal')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:09:51.167254Z","iopub.execute_input":"2022-04-18T11:09:51.16801Z","iopub.status.idle":"2022-04-18T11:09:51.177721Z","shell.execute_reply.started":"2022-04-18T11:09:51.167866Z","shell.execute_reply":"2022-04-18T11:09:51.176704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:10:05.713605Z","iopub.execute_input":"2022-04-18T11:10:05.714377Z","iopub.status.idle":"2022-04-18T11:10:05.837076Z","shell.execute_reply.started":"2022-04-18T11:10:05.71434Z","shell.execute_reply":"2022-04-18T11:10:05.836079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename column\ndf_pyspark.withColumnRenamed('species','species_type').show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:11:03.007296Z","iopub.execute_input":"2022-04-18T11:11:03.007574Z","iopub.status.idle":"2022-04-18T11:11:03.111328Z","shell.execute_reply.started":"2022-04-18T11:11:03.007543Z","shell.execute_reply":"2022-04-18T11:11:03.110502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.show(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:12:42.101329Z","iopub.execute_input":"2022-04-18T11:12:42.101665Z","iopub.status.idle":"2022-04-18T11:12:42.209706Z","shell.execute_reply.started":"2022-04-18T11:12:42.101619Z","shell.execute_reply":"2022-04-18T11:12:42.209079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basics Part 2","metadata":{}},{"cell_type":"code","source":"df_pyspark.","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:23:51.538565Z","iopub.execute_input":"2022-04-18T11:23:51.539489Z","iopub.status.idle":"2022-04-18T11:23:51.557966Z","shell.execute_reply.started":"2022-04-18T11:23:51.539417Z","shell.execute_reply":"2022-04-18T11:23:51.556709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark.filter(df_pyspark.isNotNull()).show()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:21:40.705529Z","iopub.execute_input":"2022-04-18T11:21:40.70581Z","iopub.status.idle":"2022-04-18T11:21:40.72119Z","shell.execute_reply.started":"2022-04-18T11:21:40.705778Z","shell.execute_reply":"2022-04-18T11:21:40.720503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to print null values for each column\nfrom pyspark.sql.functions import isnan, when, count, col\ndf_pyspark.select([count(when(isnan(c), c)).alias(c) for c in df_pyspark.columns]).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:29:41.704313Z","iopub.execute_input":"2022-04-18T11:29:41.704854Z","iopub.status.idle":"2022-04-18T11:29:42.181074Z","shell.execute_reply.started":"2022-04-18T11:29:41.704797Z","shell.execute_reply":"2022-04-18T11:29:42.180091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#threshold\ndf_pyspark.na.drop(how='any', thresh=3).show()\n\n#it will check if no of columsn are null more than 3 ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T11:31:33.164409Z","iopub.execute_input":"2022-04-18T11:31:33.164681Z","iopub.status.idle":"2022-04-18T11:31:33.284088Z","shell.execute_reply.started":"2022-04-18T11:31:33.164649Z","shell.execute_reply":"2022-04-18T11:31:33.283021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fill missing values\n\ndf_pyspark.na.fill('Missing Values').show()\n\n# it will replace all null/NA values with 'Missing Values'","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:05:30.686244Z","iopub.execute_input":"2022-04-18T12:05:30.687873Z","iopub.status.idle":"2022-04-18T12:05:30.970365Z","shell.execute_reply.started":"2022-04-18T12:05:30.68778Z","shell.execute_reply":"2022-04-18T12:05:30.96871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fill with Mean or any statics values\nfrom pyspark.ml.feature import Imputer\n\nimputer = Imputer(\n    inputCols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'],\n    outputCols = [\"{}_imputed\".format(c) for c in ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n).setStrategy(\"mean\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:10:01.613194Z","iopub.execute_input":"2022-04-18T12:10:01.613592Z","iopub.status.idle":"2022-04-18T12:10:01.870948Z","shell.execute_reply.started":"2022-04-18T12:10:01.61355Z","shell.execute_reply":"2022-04-18T12:10:01.869359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add imputation cols to df\nimputer.fit(df_pyspark).transform(df_pyspark).show()\n\n# as there is no null values, no it will no perform any fill condition","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:10:53.932606Z","iopub.execute_input":"2022-04-18T12:10:53.933143Z","iopub.status.idle":"2022-04-18T12:10:55.330755Z","shell.execute_reply.started":"2022-04-18T12:10:53.933103Z","shell.execute_reply":"2022-04-18T12:10:55.329615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FILTER OPERATIONS\n\n* &,|,==, ~","metadata":{}},{"cell_type":"code","source":"#df_pys = spark.read.csv('https://raw.githubusercontent.com/krishnaik06/Pyspark-With-Python/main/test1.csv',header=True)\n# it willgive  error","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:20:47.107983Z","iopub.execute_input":"2022-04-18T12:20:47.108775Z","iopub.status.idle":"2022-04-18T12:20:47.112739Z","shell.execute_reply.started":"2022-04-18T12:20:47.108718Z","shell.execute_reply":"2022-04-18T12:20:47.111823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = pd.read_csv('https://raw.githubusercontent.com/krishnaik06/Pyspark-With-Python/main/test1.csv')\nx.shape","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:18:47.237196Z","iopub.execute_input":"2022-04-18T12:18:47.23767Z","iopub.status.idle":"2022-04-18T12:18:47.481219Z","shell.execute_reply.started":"2022-04-18T12:18:47.237625Z","shell.execute_reply":"2022-04-18T12:18:47.480427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Read from API\n# https://stackoverflow.com/questions/62668884/pyspark-error-while-loading-csv-file-from-url-to-spark","metadata":{}},{"cell_type":"code","source":"url = 'https://raw.githubusercontent.com/krishnaik06/Pyspark-With-Python/main/test1.csv'\nfrom pyspark import SparkFiles\nspark.sparkContext.addFile(url)\ndf_pys = spark.read.csv(SparkFiles.get('test1.csv'), header=True) ","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:23:07.943046Z","iopub.execute_input":"2022-04-18T12:23:07.943321Z","iopub.status.idle":"2022-04-18T12:23:08.158646Z","shell.execute_reply.started":"2022-04-18T12:23:07.94329Z","shell.execute_reply":"2022-04-18T12:23:08.1577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pys.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:23:15.520732Z","iopub.execute_input":"2022-04-18T12:23:15.521013Z","iopub.status.idle":"2022-04-18T12:23:15.593672Z","shell.execute_reply.started":"2022-04-18T12:23:15.520984Z","shell.execute_reply":"2022-04-18T12:23:15.593045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filter Operations","metadata":{}},{"cell_type":"code","source":"# Salary is <= 20000\n# 1. way\ndf_pys.filter('Salary <= 20000').select(['Name','age']).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:25:04.443303Z","iopub.execute_input":"2022-04-18T12:25:04.444287Z","iopub.status.idle":"2022-04-18T12:25:04.763941Z","shell.execute_reply.started":"2022-04-18T12:25:04.444229Z","shell.execute_reply":"2022-04-18T12:25:04.762946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. way\ndf_pys.filter(df_pys['Salary']<= 20000).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:26:03.772091Z","iopub.execute_input":"2022-04-18T12:26:03.772376Z","iopub.status.idle":"2022-04-18T12:26:03.923828Z","shell.execute_reply.started":"2022-04-18T12:26:03.772347Z","shell.execute_reply":"2022-04-18T12:26:03.923183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# And --> & , or --> |\n df_pys.filter((df_pys['Salary']<= 20000) & (df_pys['Salary']>=18000)).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:27:14.542519Z","iopub.execute_input":"2022-04-18T12:27:14.54283Z","iopub.status.idle":"2022-04-18T12:27:14.682126Z","shell.execute_reply.started":"2022-04-18T12:27:14.542795Z","shell.execute_reply":"2022-04-18T12:27:14.681229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Not --> ~\ndf_pys.filter(~(df_pys['Salary']<= 20000)).show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:29:23.930175Z","iopub.execute_input":"2022-04-18T12:29:23.930482Z","iopub.status.idle":"2022-04-18T12:29:24.074685Z","shell.execute_reply.started":"2022-04-18T12:29:23.930435Z","shell.execute_reply":"2022-04-18T12:29:24.073816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"3. GroupBy & Aggregate Functions","metadata":{}},{"cell_type":"code","source":"url3 ='https://raw.githubusercontent.com/krishnaik06/Pyspark-With-Python/main/test3.csv'\nspark.sparkContext.addFile(url3)\ndf_pyspark3 = spark.read.csv(SparkFiles.get('test3.csv'), header=True, inferSchema=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:40:17.448672Z","iopub.execute_input":"2022-04-18T12:40:17.449435Z","iopub.status.idle":"2022-04-18T12:40:17.728335Z","shell.execute_reply.started":"2022-04-18T12:40:17.4494Z","shell.execute_reply":"2022-04-18T12:40:17.727536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark3.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:40:18.970659Z","iopub.execute_input":"2022-04-18T12:40:18.971008Z","iopub.status.idle":"2022-04-18T12:40:19.084447Z","shell.execute_reply.started":"2022-04-18T12:40:18.970971Z","shell.execute_reply":"2022-04-18T12:40:19.083747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_pyspark3.printSchema()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:40:20.03206Z","iopub.execute_input":"2022-04-18T12:40:20.03233Z","iopub.status.idle":"2022-04-18T12:40:20.037856Z","shell.execute_reply.started":"2022-04-18T12:40:20.032302Z","shell.execute_reply":"2022-04-18T12:40:20.037175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Group by\ndf_pyspark3.groupBy('Name').sum().show()\n\n# aggregated total salary of each person","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:42:13.242756Z","iopub.execute_input":"2022-04-18T12:42:13.243037Z","iopub.status.idle":"2022-04-18T12:42:13.456924Z","shell.execute_reply.started":"2022-04-18T12:42:13.243008Z","shell.execute_reply":"2022-04-18T12:42:13.45593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find which department gives max salary as total\ndf_pyspark3.groupBy('Departments').sum().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:44:22.232557Z","iopub.execute_input":"2022-04-18T12:44:22.232865Z","iopub.status.idle":"2022-04-18T12:44:22.492623Z","shell.execute_reply.started":"2022-04-18T12:44:22.232834Z","shell.execute_reply":"2022-04-18T12:44:22.491571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Means salary of each department\ndf_pyspark3.groupBy('Departments').mean().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:46:04.797118Z","iopub.execute_input":"2022-04-18T12:46:04.797405Z","iopub.status.idle":"2022-04-18T12:46:05.142636Z","shell.execute_reply.started":"2022-04-18T12:46:04.79737Z","shell.execute_reply":"2022-04-18T12:46:05.141702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# frequncy \n# No of teacher in each deparment\n\ndf_pyspark3.groupBy('Departments').count().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:51:35.727364Z","iopub.execute_input":"2022-04-18T12:51:35.72835Z","iopub.status.idle":"2022-04-18T12:51:36.006431Z","shell.execute_reply.started":"2022-04-18T12:51:35.728274Z","shell.execute_reply":"2022-04-18T12:51:36.005554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# How is getting max salary\n\ndf_pyspark3.groupBy('Name').max().show()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T12:52:43.531073Z","iopub.execute_input":"2022-04-18T12:52:43.531886Z","iopub.status.idle":"2022-04-18T12:52:43.783819Z","shell.execute_reply.started":"2022-04-18T12:52:43.531829Z","shell.execute_reply":"2022-04-18T12:52:43.78273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}